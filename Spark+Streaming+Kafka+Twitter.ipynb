{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/Big_Data/blob/master/Spark%2BStreaming%2BKafka%2BTwitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdM4Arzq_J0Z"
      },
      "source": [
        "*Basé sur [direct_kafka_wordcount.py](https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/direct_kafka_wordcount.py)*\n",
        "\n",
        "[`S. Alleaume`](https://github.com/crystalloide/Big_Data), 17 mai 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVyzDsvG_J0b"
      },
      "source": [
        "## Utilisation de Kafka dans un notebook Jupyter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation de trois packages essentiels:\n",
        "# - PySpark: L'interface Python pour Apache Spark, un framework de traitement de données distribuées populaire pour l'analyse de données à grande échelle.\n",
        "# - PyArrow: Une bibliothèque pour le traitement efficace de données en mémoire et la compatibilité avec le format Apache Arrow, souvent utilisée avec Spark pour des performances optimales.\n",
        "# - kafka-python: Un client Python pour Apache Kafka, permettant d'interagir avec des systèmes de messagerie en temps réel pour le streaming de données.\n",
        "\n",
        "!pip3 install pyspark pyarrow kafka-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZh_gC0QAI03",
        "outputId": "285abe0d-5ecc-47a4-bbb5-6eb59d9dc7d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (20.0.0)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.11/dist-packages (2.2.7)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "O3wgY0cg_J0b"
      },
      "outputs": [],
      "source": [
        "# Version du connecteur Kafka: spark-sql-kafka-0-10 est la version plus récente et recommandée pour travailler avec Kafka dans Spark. La version 0-8 est obsolète et n'est plus maintenue.\n",
        "# Version Scala: 2.12 est compatible avec les versions récentes de Spark.\n",
        "# Version de Spark: version 3.3.0 compatible avec les environnements Colab actuels.\n",
        "\n",
        "# Cette configuration permet d'utiliser l'API Structured Streaming de Spark avec Kafka,\n",
        "# qui offre une meilleure performance et une API plus simple que l'ancien Spark Streaming DStream utilisé précédemment\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwJNco8I_J0c"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "RyxCQHFQ_J0c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Remarque : SparkContext et SparkConf sont remplacés par SparkSession, point d'entrée recommandé pour les applications Spark modernes.\n",
        "# Les classes StreamingContext et KafkaUtils de l'ancien module pyspark.streaming.kafka ne sont plus utilisées dans l'API Structured Streaming.\n",
        "# Importations pour pyspark.sql.functions et pyspark.sql.types, essentielles pour travailler avec l'API DataFrame et Structured Streaming.\n",
        "\n",
        "# Avec cette nouvelle configuration, vous pourrez créer un pipeline de streaming de données plus robuste et maintenable,\n",
        "# avec un meilleur support pour les traitements à base de SQL et une meilleure gestion des garanties de livraison des messages.\n",
        "# À l'étape suivante, il faudra  créer une SparkSession et configurer une source de streaming Kafka.\n",
        "\n",
        "# Print to stdout\n",
        "from __future__ import print_function\n",
        "\n",
        "# Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# json parsing\n",
        "import json\n",
        "\n",
        "# Pour utiliser des fonctions SQL et des opérations sur les colonnes\n",
        "from pyspark.sql.functions import col, from_json, explode\n",
        "from pyspark.sql.types import StructType, StringType, StructField"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmGKlqPZ_J0c"
      },
      "source": [
        "## Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MKx0_9TH_J0c"
      },
      "outputs": [],
      "source": [
        "# Création d'une SparkSession plutôt qu'un SparkContext\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PythonStructuredStreamingKafkaWordCount\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c947pdZa_J0c"
      },
      "source": [
        "## Create Streaming context, with a batch duration of 10 seconds\n",
        "\n",
        "* http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext\n",
        "* http://spark.apache.org/docs/latest/streaming-programming-guide.html#initializing-streamingcontext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfz6Qasb_J0d"
      },
      "outputs": [],
      "source": [
        "# Avec l'API Structured Streaming moderne, plus besoin de créer un StreamingContext.\n",
        "# L'intervalle de traitement sera défini lors de la configuration du streaming\n",
        "\n",
        "# Optionnel: définir des configurations de log pour réduire la verbosité\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Changements importants:\n",
        "\n",
        "# Suppression de la création du StreamingContext qui n'est plus utilisé dans l'API Structured Streaming.\n",
        "# Avec l'API Structured Streaming, le concept d'intervalle de traitement par lots (par exemple : 10 secondes)\n",
        "# est remplacé par les modes de traitement déclenchés (trigger modes) qui sont définis au moment de démarrer la requête de streaming.\n",
        "\n",
        "# La nouvelle approche est plus flexible car permet de:\n",
        "# - Configurer le mode de traitement des données (trigger) au moment de la définition de la requête\n",
        "# - Faire des mises à jour continues sans microbatches (mode continu)\n",
        "# - Ou définir un intervalle de traitement par lots similaire à l'ancien modèle\n",
        "\n",
        "# Plus tard, lorsque vous définirez votre requête de streaming,\n",
        "# vous pourrez spécifier un mode de déclenchement équivalent à l'ancien intervalle de 10 secondes avec par exemple :\n",
        "\n",
        "# Utilisation lors de la configuration d'une requête de streaming\n",
        "query = df.writeStream \\\n",
        "    .trigger(processingTime='10 seconds') \\\n",
        "    .format(\"...\") \\\n",
        "    .start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4zozBmP_J0d"
      },
      "source": [
        "## Connect to Kafka\n",
        "\n",
        "Topic `twitter`, Consumer group `spark-streaming`\n",
        "\n",
        "* Voici la documentation à jour pour l'intégration Structured Streaming avec Kafka:\n",
        "\n",
        "Pour Spark 3.x: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### L'ancienne API Spark Streaming a été remplacée pour utiliser l'API Structured Streaming moderne avec le nouveau connecteur Kafka.\n",
        "\n",
        "#### Définition des options Kafka pour la lecture en streaming :\n",
        "\n",
        "kafka_options = {\n",
        "    \"kafka.bootstrap.servers\": \"cdh57-01-node-01.moffatt.me:9092\",\n",
        "    \"subscribe\": \"twitter\",\n",
        "    \"startingOffsets\": \"latest\",\n",
        "    \"group.id\": \"spark-streaming\"\n",
        "}\n",
        "\n",
        "####  Création d'un DataFrame de streaming depuis Kafka :\n",
        "kafkaDF = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafka_options) \\\n",
        "    .load()\n",
        "\n",
        "#### Convertion de la valeur des messages Kafka (en binaire) en chaîne de caractères :\n",
        "valueDF = kafkaDF.selectExpr(\"CAST(value AS STRING)\")\n",
        "\n",
        "### Modifications importantes:\n",
        "\n",
        "#### Remplacement de l'adresse Zookeeper (port 2181) par l'adresse des brokers Kafkabrokers Kafka (port 9092)\n",
        "#### Avcec les protocole Kraft, le connecteur Kafka se connecte directement aux brokers Kafka sans passer par Zookeeper\n",
        "\n",
        "### API:\n",
        "#### KafkaUtils.createStream() est remplacé par l'API DataFrame avec spark.readStream.format(\"kafka\")\n",
        "#### L'API moderne est plus facile à utiliser et offre de meilleures garanties de traitement\n",
        "\n",
        "\n",
        "### Configuration:\n",
        "#### Utilisation de startingOffsets pour définir à partir de quel offset lire (équivalent du comportement de l'ancien consommateur)\n",
        "#### Le groupe de consommateurs est maintenant configuré via l'option group.id\n",
        "\n",
        "\n",
        "### Traitement des données:\n",
        "\n",
        "#### Les messages Kafka sont lus sous forme de DataFrame avec des colonnes standardisées (key, value, topic, partition, offset, timestamp, etc.)\n",
        "#### Une étape de conversion est nécessaire pour transformer les valeurs binaires en chaînes de caractères"
      ],
      "metadata": {
        "id": "Y4ZOBi3nFD6D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5kNgcCK_J0d"
      },
      "outputs": [],
      "source": [
        "kafkaStream = KafkaUtils.createStream(ssc, 'cdh57-01-node-01.moffatt.me:2181', 'spark-streaming', {'twitter':1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIUYZyfE_J0e"
      },
      "source": [
        "## Parse the inbound message as json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oznXxVlX_J0e"
      },
      "outputs": [],
      "source": [
        "parsed = kafkaStream.map(lambda v: json.loads(v[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPEzjDNH_J0e"
      },
      "source": [
        "## Count the number of instance of each tweet text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YsCsv-nr_J0e"
      },
      "outputs": [],
      "source": [
        "text_counts = parsed.map(lambda tweet: (tweet['text'],1)).\\\n",
        "  reduceByKey(lambda x,y: x + y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGnKG3UL_J0e"
      },
      "source": [
        "### Print the text counts (first ten shown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aeDOez4r_J0e"
      },
      "outputs": [],
      "source": [
        "text_counts.pprint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq5twSva_J0e"
      },
      "source": [
        "## Count the number of tweets per author"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1-xny014_J0f"
      },
      "outputs": [],
      "source": [
        "author_counts = parsed.map(lambda tweet: (tweet['user']['screen_name'],1)).\\\n",
        "  reduceByKey(lambda x,y: x + y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfxW9k9R_J0f"
      },
      "source": [
        "### Print the author tweet counts (first ten shown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fOlpfVM8_J0f"
      },
      "outputs": [],
      "source": [
        "author_counts.pprint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6Y9RAvk_J0f"
      },
      "source": [
        "## Start the streaming context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBc3YnwV_J0f",
        "outputId": "a7740a43-b275-4c66-e711-3f997859e389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:28:20\n",
            "-------------------------------------------\n",
            "(u'Rex Tillerson cuts ties with Exxon Mobil via $180m retirement package: Donald Trump\\u2019s nominee for\\u2026 https://t.co/eONdIe8xvL | #Election2016', 1)\n",
            "(u'Rex Tillerson cuts ties with Exxon Mobil via $180m retirement package: Donald Trump\\u2019s nominee for\\u2026 https://t.co/NC6njITxcE | #Election2016', 1)\n",
            "(u'A new approach to moving ore from remote mines: #airships! https://t.co/HmuoGQjEzC', 1)\n",
            "(u'Rex Tillerson cuts ties with Exxon Mobil via $180m retirement package: Donald Trump\\u2019s nominee for\\u2026 https://t.co/sk05rwzxMa | #Election2016', 1)\n",
            "(u'Donald Trump appears to trust Julian Assange more than US intelligence agents https://t.co/ShlmBXoW4Z #election2016 https://t.co/HS95S98fOS', 1)\n",
            "(u'Ngopo2 kudu dipikir disik. Ojo nganti kowe kagol amargo opo sing tok lakoni kuwi ora di pikir sek.', 1)\n",
            "(u'Rex Tillerson cuts ties with Exxon Mobil via $180m retirement package: Donald Trump\\u2019s nominee for\\u2026 https://t.co/f94cqyq06e | #Election2016', 1)\n",
            "(u'Mengagendakan jalan-jalan sendiri di akhir pekan nanti. Kudu kelakon ya Ga biar ngga rencana mulu dari jaman-jaman penat UAS.', 1)\n",
            "(u'Rex Tillerson cuts ties with Exxon Mobil via $180m retirement package: Donald Trump\\u2019s nominee for\\u2026 https://t.co/YpWSL6qxia | #Election2016', 1)\n",
            "(u'Rex Tillerson cuts ties with Exxon Mobil via $180m retirement package: Donald Trump\\u2019s nominee for\\u2026 https://t.co/quHaez3x4q | #Election2016', 1)\n",
            "...\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:28:20\n",
            "-------------------------------------------\n",
            "(u'POLSRochester', 1)\n",
            "(u'votebotvotebot', 1)\n",
            "(u'dbiservices', 1)\n",
            "(u'CintraSoftware', 1)\n",
            "(u'POLSPittsburgh', 1)\n",
            "(u'tasyarozli', 1)\n",
            "(u'tweetazsugar', 1)\n",
            "(u'POLSDesMoines', 1)\n",
            "(u'POLSHartford', 1)\n",
            "(u'RebellionRider', 1)\n",
            "...\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:28:30\n",
            "-------------------------------------------\n",
            "(u'Technical Project Manager - (Java, Websphere, Oracle Apps DBA ) (#Harrisburg,  PA) #job#ProjectManager https://t.co/pWcekZcRxS', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:28:30\n",
            "-------------------------------------------\n",
            "(u'JCjobPOSTINGS', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:28:40\n",
            "-------------------------------------------\n",
            "(u'#ImWithHer #Election2016 #AMJOY #Decision2016 Sad but true. Will PEOTUS pardon Snowden &amp; Manning too?\\U0001f4af\\U0001f612\\U0001f644\\U0001f622 https://t.co/THaKoYb3dS', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:28:40\n",
            "-------------------------------------------\n",
            "(u'ReneeB_75', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:28:50\n",
            "-------------------------------------------\n",
            "(u'RT @byTurgutUyar: \"Yine ku\\u015fkuya d\\xfc\\u015fecek kadar \\xfc\\u015f\\xfcyorum.\"- Franz Kafka', 1)\n",
            "(u'RT @miabuelasabia: S\\xe9 feliz con lo que tienes en tus brazos, porque un d\\xeda puedes cargar el vac\\xedo\\n(Franz Kafka)', 1)\n",
            "(u'RT @EXASOLAG: Performance comparison between #EXASOL &amp; #Oracle In-Memory Option - https://t.co/rpu6F37H0S (in Russian) #bigdata #analytics\\u2026', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:28:50\n",
            "-------------------------------------------\n",
            "(u'OscaryMadriz', 1)\n",
            "(u'aylinyigit6', 1)\n",
            "(u'EXAGolo', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:00\n",
            "-------------------------------------------\n",
            "(u'@Snoopy not move to Canada.', 1)\n",
            "(u'_ https://t.co/V9KoFbSQ5G #Samsung #Oracle #TipsAndTricks #HTML5 9413', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:00\n",
            "-------------------------------------------\n",
            "(u'eddie8374', 1)\n",
            "(u'cn1bot3', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:10\n",
            "-------------------------------------------\n",
            "(u'RT @OUPAcademic: The role of family values in the 2016 presidential election https://t.co/AYoizr7poo Via @OUPReligion #Election2016', 1)\n",
            "(u'NEED ONLY USC AND GC :- Oracle EBS Finance Developer__Phoenix, AZ https://t.co/XSlvp7Ny2N', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:10\n",
            "-------------------------------------------\n",
            "(u'academicpresses', 1)\n",
            "(u'hireitpeople', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:20\n",
            "-------------------------------------------\n",
            "(u'VIDEO: Paul Ryan Stops Lawmaker\\u2019s Son from \\u2018Dabbing\\u2019 During Swearing-In Ceremony https://t.co/DC9gebMymB #tcot\\u2026 https://t.co/MAJZ6jDSnT', 1)\n",
            "(u'.@GOP @SpeakerRyan @SenateMajLdr @SenateGOP @HouseGOP\\n#Election2016 Mandate\\n#ElectoralCollege Landslide\\u2026 https://t.co/dRx5jJKBIc', 1)\n",
            "(u'Lalu salibis, zionis dan Syiah Rafidhah gak bahaya gitu? Jd Erdogan boleh mesra dgn mrk? https://t.co/ACWOUTA1kC', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:20\n",
            "-------------------------------------------\n",
            "(u'Makibao008', 1)\n",
            "(u'TrumpCard555', 1)\n",
            "(u'cafenetamerica', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:30\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:30\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:40\n",
            "-------------------------------------------\n",
            "(u'RT @orclapexblogs: Oracle ACE Alumnus - so long, and thanks for all the fish https://t.co/vmHTphn2Z4 #OrclAPEX #odtug', 1)\n",
            "(u'IM NOT READY FOR BUNKASAI, UWAA NISA KUDU OTTOKHAE? &gt;.&lt;', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:40\n",
            "-------------------------------------------\n",
            "(u'lbrizzi', 1)\n",
            "(u'annisahael', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:50\n",
            "-------------------------------------------\n",
            "(u'#empleo #IT Analista T\\xe9cnico Oracle - Madrid https://t.co/tmSgkAKO0d', 1)\n",
            "(u'RT @booknerdfession: \\u201cMemories warm you up from the inside. But they also tear you apart.\\u201d \\n\\u2015 Haruki Murakami, Kafka on the Shore', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:29:50\n",
            "-------------------------------------------\n",
            "(u'MalikAqsa', 1)\n",
            "(u'IJInformatico', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:30:00\n",
            "-------------------------------------------\n",
            "(u'.@Harlan #Election2016 left US with the same unaccountable dysfunctional party duopoly. Join me to invoke reform. #iamameriCAN #FirstCitizen', 1)\n",
            "(u'RT @bendawla: kafka demi\\u015f ki ah milena sen ba\\u015fkayd\\u0131n hasta bir adam\\u0131 sevecek kadar hastayd\\u0131n milena da demi\\u015f ne hastas\\u0131 ya makyaj yapmad\\u0131m\\u2026', 1)\n",
            "(u\"@FiiDee_ mtu hawezi kudu any. I'm here\\U0001f606\", 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2017-01-04 14:30:00\n",
            "-------------------------------------------\n",
            "(u'Ecenurztrk', 1)\n",
            "(u'mejar412', 1)\n",
            "(u'1stcitizen', 1)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ssc.start()\n",
        "ssc.awaitTermination()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}